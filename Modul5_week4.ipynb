{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpT/0rMPJiJq/Mzyu966Gn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thuyk39nguyen/AIO/blob/main/Modul5_week4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fTdbYmfRpuVE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bài 1 Gradient Descent:**"
      ],
      "metadata": {
        "id": "QeyqZezO1WNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_w(W):\n",
        "  \"\"\" Thực hiện tính toán gradient của dw1 và dw2\n",
        "  Argument:\n",
        "  W--np.array[w1,w2]\n",
        "  Returns:\n",
        "  dW-- np.array[dw1,dw2], array chứa giá trị đạo hàm theo w1, w2\n",
        "  \"\"\"\n",
        "  dw1 = 2 * W[0]\n",
        "  dw2 = 2 * W[1]\n",
        "  dW = np.array([dw1, dw2])\n",
        "  return dW"
      ],
      "metadata": {
        "id": "s5I-UFEs0r-o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.array([-5.0, -2.0])  # Điểm khởi tạo\n",
        "gradient = df_w(W)\n",
        "print(f\"Gradient tại W = {W} là: {gradient}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHwCO6h90xvq",
        "outputId": "8e3ee9e8-fbe2-43f8-9d21-c40974dfadf2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient tại W = [-5. -2.] là: [-10.  -4.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(W, dW, lr):\n",
        "    \"\"\"\n",
        "    Thực hiện thuật toán Gradient Descent để update w1 và w2\n",
        "    Arguments:\n",
        "    W -- np.array: [w1, w2]\n",
        "    dW -- np.array: [dw1, dw2], array chứa giá trị đạo hàm theo w1 và w2\n",
        "    lr -- float: learning rate\n",
        "    Returns:\n",
        "    W -- np.array: [w1, w2] w1 và w2 sau khi đã update\n",
        "    \"\"\"\n",
        "    # Công thức cập nhật:\n",
        "    W = W - lr * dW\n",
        "    return W"
      ],
      "metadata": {
        "id": "kzqqbzfPwonX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.array([-5.0, -2.0])  # Điểm khởi tạo\n",
        "dW = df_w(W)\n",
        "print(f\"Gradient tại W = {W} là: {dW}\")\n",
        "lr = 0.4  # Learning rate\n",
        "\n",
        "# Cập nhật tham số\n",
        "W_updated = sgd(W, dW, lr)\n",
        "print(f\"Tham số sau khi cập nhật: {W_updated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-564aBOxEbw",
        "outputId": "217389ad-e65d-43be-ee45-3e38aa140d8d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient tại W = [-5. -2.] là: [-10.  -4.]\n",
            "Tham số sau khi cập nhật: [-1.  -0.4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pl(optimizer, lr, epochs):\n",
        "    \"\"\"\n",
        "    Thực hiện tìm điểm minimum của function (1) dựa vào thuật toán\n",
        "    được truyền vào từ optimizer\n",
        "    Arguments:\n",
        "    optimizer -- function thực hiện thuật toán optimization cụ thể\n",
        "    lr -- float: learning rate\n",
        "    epochs -- int: số lượng lần (epoch) lặp để tìm điểm minimum\n",
        "    Returns:\n",
        "    results -- list: list các cặp điểm [w1, w2] sau mỗi epoch (mỗi lần cập nhật)\n",
        "    \"\"\"\n",
        "    # initial point\n",
        "    W = np.array([-5, -2], dtype=np.float32)\n",
        "    # list of results\n",
        "    results = [W]\n",
        "\n",
        "    ################ YOUR CODE HERE ####################\n",
        "    for epoch in range(epochs):\n",
        "        # Tính gradient tại điểm hiện tại\n",
        "        dW = df_w(W)\n",
        "\n",
        "        # Cập nhật W bằng optimizer\n",
        "        W = optimizer(W, dW, lr)\n",
        "\n",
        "        # Lưu lại W sau mỗi epoch\n",
        "        results.append(W)\n",
        "\n",
        "        # In ra\n",
        "        print(f\"Epoch {epoch}: W = {W}\")\n",
        "    ###################################################\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "QRR-_mP8xKJp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gọi hàm train_pl\n",
        "lr = 0.4\n",
        "epochs = 30\n",
        "results = train_pl(optimizer=sgd, lr=lr, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7Ow9uw2yxb7",
        "outputId": "49b38150-b1f4-4014-ce94-5fc4469f2f98"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: W = [-1.  -0.4]\n",
            "Epoch 1: W = [-0.2  -0.08]\n",
            "Epoch 2: W = [-0.04  -0.016]\n",
            "Epoch 3: W = [-0.008  -0.0032]\n",
            "Epoch 4: W = [-0.0016  -0.00064]\n",
            "Epoch 5: W = [-0.00032  -0.000128]\n",
            "Epoch 6: W = [-6.40e-05 -2.56e-05]\n",
            "Epoch 7: W = [-1.28e-05 -5.12e-06]\n",
            "Epoch 8: W = [-2.560e-06 -1.024e-06]\n",
            "Epoch 9: W = [-5.120e-07 -2.048e-07]\n",
            "Epoch 10: W = [-1.024e-07 -4.096e-08]\n",
            "Epoch 11: W = [-2.048e-08 -8.192e-09]\n",
            "Epoch 12: W = [-4.0960e-09 -1.6384e-09]\n",
            "Epoch 13: W = [-8.1920e-10 -3.2768e-10]\n",
            "Epoch 14: W = [-1.6384e-10 -6.5536e-11]\n",
            "Epoch 15: W = [-3.27680e-11 -1.31072e-11]\n",
            "Epoch 16: W = [-6.55360e-12 -2.62144e-12]\n",
            "Epoch 17: W = [-1.31072e-12 -5.24288e-13]\n",
            "Epoch 18: W = [-2.621440e-13 -1.048576e-13]\n",
            "Epoch 19: W = [-5.242880e-14 -2.097152e-14]\n",
            "Epoch 20: W = [-1.048576e-14 -4.194304e-15]\n",
            "Epoch 21: W = [-2.097152e-15 -8.388608e-16]\n",
            "Epoch 22: W = [-4.1943040e-16 -1.6777216e-16]\n",
            "Epoch 23: W = [-8.3886080e-17 -3.3554432e-17]\n",
            "Epoch 24: W = [-1.6777216e-17 -6.7108864e-18]\n",
            "Epoch 25: W = [-3.35544320e-18 -1.34217728e-18]\n",
            "Epoch 26: W = [-6.71088640e-19 -2.68435456e-19]\n",
            "Epoch 27: W = [-1.34217728e-19 -5.36870912e-20]\n",
            "Epoch 28: W = [-2.68435456e-20 -1.07374182e-20]\n",
            "Epoch 29: W = [-5.36870912e-21 -2.14748365e-21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bài 2 Gradient Descent + Momentum:**"
      ],
      "metadata": {
        "id": "u_HUrseazRT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_w(w):\n",
        "    w1, w2 = w\n",
        "    dw1 = 2 * w1  # Example gradient for some quadratic function\n",
        "    dw2 = 2 * w2  # Change this based on your function\n",
        "    return np.array([dw1, dw2])"
      ],
      "metadata": {
        "id": "eUD5M00k1c5n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform SGD with Momentum\n",
        "def sgd_momentum(w, v, lr, beta):\n",
        "    dw = df_w(w)  # Gradient\n",
        "    v = beta * v + (1 - beta) * dw  # Update velocity\n",
        "    w = w - lr * v  # Update weights\n",
        "    return w, v"
      ],
      "metadata": {
        "id": "NE3_k_rb0nqZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_p1(learning_rate, epochs, beta=0.5):\n",
        "    # Initial values\n",
        "    w = np.array([-5.0, -2.0])  # Starting point\n",
        "    v = np.array([0.0, 0.0])    # Initial velocity\n",
        "    results = [w.copy()]        # List to store weight updates\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        w, v = sgd_momentum(w, v, learning_rate, beta)  # Update w and v\n",
        "        results.append(w.copy())  # Store current weights\n",
        "        print(f\"Epoch {epoch}: w = {w}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "t_bu9fJJzTCJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "learning_rate = 0.6\n",
        "epochs = 30\n",
        "\n",
        "# Run the training\n",
        "results = train_p1(learning_rate, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdHohse-4Eaa",
        "outputId": "3fae378f-4b54-408a-b445-111b34907496"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: w = [-2.  -0.8]\n",
            "Epoch 1: w = [0.7  0.28]\n",
            "Epoch 2: w = [1.63  0.652]\n",
            "Epoch 3: w = [1.117  0.4468]\n",
            "Epoch 4: w = [0.1903  0.07612]\n",
            "Epoch 5: w = [-0.38723  -0.154892]\n",
            "Epoch 6: w = [-0.443657  -0.1774628]\n",
            "Epoch 7: w = [-0.2056763  -0.08227052]\n",
            "Epoch 8: w = [0.03671983 0.01468793]\n",
            "Epoch 9: w = [0.135886  0.0543544]\n",
            "Epoch 10: w = [0.10393748 0.04157499]\n",
            "Epoch 11: w = [0.02560074 0.01024029]\n",
            "Epoch 12: w = [-0.02892808 -0.01157123]\n",
            "Epoch 13: w = [-0.03883564 -0.01553426]\n",
            "Epoch 14: w = [-0.02048804 -0.00819521]\n",
            "Epoch 15: w = [0.00097859 0.00039144]\n",
            "Epoch 16: w = [0.01112475 0.0044499 ]\n",
            "Epoch 17: w = [0.00952298 0.00380919]\n",
            "Epoch 18: w = [0.00300831 0.00120332]\n",
            "Epoch 19: w = [-0.00205401 -0.00082161]\n",
            "Epoch 20: w = [-0.00335276 -0.00134111]\n",
            "Epoch 21: w = [-0.00199048 -0.00079619]\n",
            "Epoch 22: w = [-1.15051416e-04 -4.60205664e-05]\n",
            "Epoch 23: w = [0.00089169 0.00035668]\n",
            "Epoch 24: w = [0.00086005 0.00034402]\n",
            "Epoch 25: w = [0.0003282  0.00013128]\n",
            "Epoch 26: w = [-1.34646838e-04 -5.38587353e-05]\n",
            "Epoch 27: w = [-0.00028528 -0.00011411]\n",
            "Epoch 28: w = [-1.89429854e-04 -7.57719415e-05]\n",
            "Epoch 29: w = [-2.78461612e-05 -1.11384645e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bài 3 RMSProp:**"
      ],
      "metadata": {
        "id": "ErnV4r3P4zpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(w):\n",
        "    w1, w2 = w\n",
        "    dw1 = 2 * w1\n",
        "    dw2 = 2 * w2\n",
        "    return np.array([dw1, dw2])"
      ],
      "metadata": {
        "id": "EGXdgQ0M4dGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSProp optimizer\n",
        "def rmsprop(w, s, lr, gamma, epsilon=1e-6):\n",
        "    dw = gradient(w)\n",
        "    s = gamma * s + (1 - gamma) * dw**2  # Update squared gradient accumulator\n",
        "    w = w - lr * dw / np.sqrt(s + epsilon)  # Update weights\n",
        "    return w, s"
      ],
      "metadata": {
        "id": "MggjRQM09Ra2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters\n",
        "w_rms = np.array([-5.0, -2.0])\n",
        "s_rms = np.array([0.0, 0.0])\n",
        "lr = 0.3\n",
        "gamma = 0.9\n",
        "epochs = 30\n",
        "\n",
        "# Training with RMSProp\n",
        "results_rms = []\n",
        "\n",
        "for t in range(1, epochs + 1):\n",
        "    w_rms, s_rms = rmsprop(w_rms, s_rms, lr, gamma)\n",
        "    results_rms.append(w_rms.copy())\n",
        "    print(f\"Epoch {t}: w_rms = {w_rms}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo29G6ar9eje",
        "outputId": "d180675a-a39c-4666-ff04-5b1682d9a299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w_rms = [-4.05131675 -1.051317  ]\n",
            "Epoch 2: w_rms = [-3.4351913  -0.59152367]\n",
            "Epoch 3: w_rms = [-2.9589299  -0.32943961]\n",
            "Epoch 4: w_rms = [-2.56545541 -0.17756498]\n",
            "Epoch 5: w_rms = [-2.22919779 -0.09163267]\n",
            "Epoch 6: w_rms = [-1.93625965 -0.04494506]\n",
            "Epoch 7: w_rms = [-1.67816897 -0.02081427]\n",
            "Epoch 8: w_rms = [-1.44934202 -0.00903561]\n",
            "Epoch 9: w_rms = [-1.2458743  -0.00364592]\n",
            "Epoch 10: w_rms = [-1.06489551 -0.00135352]\n",
            "Epoch 11: w_rms = [-9.04195011e-01 -4.56446850e-04]\n",
            "Epoch 12: w_rms = [-7.61989552e-01 -1.37563852e-04]\n",
            "Epoch 13: w_rms = [-6.36771909e-01 -3.62604129e-05]\n",
            "Epoch 14: w_rms = [-5.27209041e-01 -8.11346467e-06]\n",
            "Epoch 15: w_rms = [-4.32072737e-01 -1.47475580e-06]\n",
            "Epoch 16: w_rms = [-3.50193194e-01 -2.02788087e-07]\n",
            "Epoch 17: w_rms = [-2.80429810e-01 -1.84236684e-08]\n",
            "Epoch 18: w_rms = [-2.21655480e-01 -7.67784339e-10]\n",
            "Epoch 19: w_rms = [-1.72751643e-01  7.80403953e-12]\n",
            "Epoch 20: w_rms = [-1.32611743e-01 -5.05753004e-13]\n",
            "Epoch 21: w_rms = [-1.00150851e-01  6.19064314e-14]\n",
            "Epoch 22: w_rms = [-7.43192815e-02 -1.13361800e-14]\n",
            "Epoch 23: w_rms = [-5.41180470e-02  2.80134662e-15]\n",
            "Epoch 24: w_rms = [-3.86142076e-02 -8.81233396e-16]\n",
            "Epoch 25: w_rms = [-2.69544316e-02  3.39876975e-16]\n",
            "Epoch 26: w_rms = [-1.83754796e-02 -1.56560243e-16]\n",
            "Epoch 27: w_rms = [-1.22107744e-02  8.44872832e-17]\n",
            "Epoch 28: w_rms = [-7.89269047e-03 -5.26296600e-17]\n",
            "Epoch 29: w_rms = [-4.95064359e-03  3.74048305e-17]\n",
            "Epoch 30: w_rms = [-3.00544474e-03 -3.00455784e-17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bài 4 Adam:**"
      ],
      "metadata": {
        "id": "JfzYvf_U-doV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(w):\n",
        "    w1, w2 = w\n",
        "    dw1 = 2 * w1\n",
        "    dw2 = 2 * w2\n",
        "    return np.array([dw1, dw2])"
      ],
      "metadata": {
        "id": "bvn96veu-cxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam optimizer\n",
        "def adam(w, m, v, lr, beta1, beta2, t, epsilon=1e-8):\n",
        "    dw = gradient(w)\n",
        "    m = beta1 * m + (1 - beta1) * dw  # Update first moment\n",
        "    v = beta2 * v + (1 - beta2) * dw**2  # Update second moment\n",
        "    m_hat = m / (1 - beta1**t)  # Bias correction for m\n",
        "    v_hat = v / (1 - beta2**t)  # Bias correction for v\n",
        "    w = w - lr * m_hat / (np.sqrt(v_hat) + epsilon)  # Update weights\n",
        "    return w, m, v"
      ],
      "metadata": {
        "id": "67TdYPRF-rjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_adam = np.array([-5.0, -2.0])\n",
        "m_adam = np.array([0.0, 0.0])\n",
        "v_adam = np.array([0.0, 0.0])\n",
        "\n",
        "lr = 0.2\n",
        "gamma = 0.9\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epochs = 30\n",
        "\n",
        "# Training with Adam\n",
        "results_adam = []\n",
        "\n",
        "for t in range(1, epochs + 1):\n",
        "    w_adam, m_adam, v_adam = adam(w_adam, m_adam, v_adam, lr, beta1, beta2, t)\n",
        "    results_adam.append(w_adam.copy())\n",
        "    print(f\"Epoch {t}: w_adam = {w_adam}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECVVVVH7-trE",
        "outputId": "698292ea-447f-443a-e60b-87cb17204e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w_adam = [-4.8 -1.8]\n",
            "Epoch 2: w_adam = [-4.60025438 -1.60082446]\n",
            "Epoch 3: w_adam = [-4.40094787 -1.40317254]\n",
            "Epoch 4: w_adam = [-4.20227682 -1.20787812]\n",
            "Epoch 5: w_adam = [-4.0044493  -1.01592732]\n",
            "Epoch 6: w_adam = [-3.80768514 -0.82847291]\n",
            "Epoch 7: w_adam = [-3.61221586 -0.64684141]\n",
            "Epoch 8: w_adam = [-3.41828455 -0.47252744]\n",
            "Epoch 9: w_adam = [-3.2261455  -0.30716912]\n",
            "Epoch 10: w_adam = [-3.03606381 -0.15249831]\n",
            "Epoch 11: w_adam = [-2.84831474 -0.010263  ]\n",
            "Epoch 12: w_adam = [-2.66318291  0.11787579]\n",
            "Epoch 13: w_adam = [-2.48096127  0.23046188]\n",
            "Epoch 14: w_adam = [-2.30194986  0.32635896]\n",
            "Epoch 15: w_adam = [-2.1264543   0.40484218]\n",
            "Epoch 16: w_adam = [-1.95478401  0.46564982]\n",
            "Epoch 17: w_adam = [-1.78725021  0.50898818]\n",
            "Epoch 18: w_adam = [-1.62416361  0.53549457]\n",
            "Epoch 19: w_adam = [-1.46583185  0.54617155]\n",
            "Epoch 20: w_adam = [-1.31255673  0.54230819]\n",
            "Epoch 21: w_adam = [-1.16463119  0.52540208]\n",
            "Epoch 22: w_adam = [-1.02233618  0.49709058]\n",
            "Epoch 23: w_adam = [-0.88593737  0.45909504]\n",
            "Epoch 24: w_adam = [-0.75568183  0.41317771]\n",
            "Epoch 25: w_adam = [-0.6317948   0.36110877]\n",
            "Epoch 26: w_adam = [-0.51447647  0.30464031]\n",
            "Epoch 27: w_adam = [-0.40389904  0.24548389]\n",
            "Epoch 28: w_adam = [-0.30020401  0.18528896]\n",
            "Epoch 29: w_adam = [-0.20349989  0.1256205 ]\n",
            "Epoch 30: w_adam = [-0.11386026  0.06793504]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bài 5 Vanishing Problem (Optional):**"
      ],
      "metadata": {
        "id": "NiM1UgDf_08P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the FashionMNIST dataset\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n"
      ],
      "metadata": {
        "id": "3cAmb3m9A7Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters\n",
        "batch_size = 512\n",
        "num_epochs = 300\n",
        "lr = 0.05"
      ],
      "metadata": {
        "id": "Jwslzs6q_3HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformation\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Training dataset and DataLoader\n",
        "train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Test dataset and DataLoader\n",
        "test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOW9aubHA3rN",
        "outputId": "43a6c3f5-f7d0-4719-b8f2-13b487e6671e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 16.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 250kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 4.61MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.71MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
        "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.output = nn.Linear(hidden_dims, output_dims)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.Flatten()(x)  # Flatten the input\n",
        "        x = self.layer1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.sigmoid(x)\n",
        "        out = self.output(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "yziec7WfCIN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Define the model\n",
        "model = MLP(input_dims=784, hidden_dims=128, output_dims=10).to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "0ZrzX7UICqip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to track losses and accuracies\n",
        "train_losses = []\n",
        "train_acc = []\n",
        "val_losses = []\n",
        "val_acc = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    t_loss = 0\n",
        "    t_acc = 0\n",
        "    cnt = 0\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device), y.to(device)  # Move data to device\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(X)  # Forward pass\n",
        "        loss = criterion(outputs, y)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "        t_loss += loss.item()  # Accumulate training loss\n",
        "        t_acc += (torch.argmax(outputs, dim=1) == y).sum().item()  # Accumulate accuracy\n",
        "        cnt += len(y)  # Count samples\n",
        "    t_loss /= len(train_loader)  # Average training loss\n",
        "    train_losses.append(t_loss)\n",
        "    t_acc /= cnt  # Average training accuracy\n",
        "    train_acc.append(t_acc)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    v_loss = 0\n",
        "    v_acc = 0\n",
        "    cnt = 0\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(device), y.to(device)  # Move data to device\n",
        "            outputs = model(X)  # Forward pass\n",
        "            loss = criterion(outputs, y)  # Compute loss\n",
        "            v_loss += loss.item()  # Accumulate validation loss\n",
        "            v_acc += (torch.argmax(outputs, dim=1) == y).sum().item()  # Accumulate accuracy\n",
        "            cnt += len(y)  # Count samples\n",
        "    v_loss /= len(test_loader)  # Average validation loss\n",
        "    val_losses.append(v_loss)\n",
        "    v_acc /= cnt  # Average validation accuracy\n",
        "    val_acc.append(v_acc)"
      ],
      "metadata": {
        "id": "6upaorWWDex3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Losses:\", train_losses)\n",
        "print(\"Training Accuracies:\", train_acc)\n",
        "print(\"Validation Losses:\", val_losses)\n",
        "print(\"Validation Accuracies:\", val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLmKn8R6Dr8m",
        "outputId": "694c9384-717d-402c-f216-ef142cda7403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Losses: [2.3091432563329146, 2.302774615206961, 2.3028013645592385, 2.30281538882498, 2.3028021949832724, 2.3028034739575145, 2.302803504264961, 2.302761514308089, 2.3027494600263694, 2.30278456817239, 2.3027864735005266, 2.3028037487450295, 2.302757913783445, 2.302804106372898, 2.3027985358642318, 2.302830261699224, 2.3028037689499934, 2.3027863340862726, 2.302829067585832, 2.3027853905144386, 2.3028244911614113, 2.3027604818344116, 2.3027482578309915, 2.302792229894864, 2.30285073538958, 2.3028028960955345, 2.302823155613269, 2.302782515348014, 2.302833229808484, 2.302789777012195, 2.30280699972379, 2.302813346103086, 2.3028071654044977, 2.3027976145178584, 2.3027801877361234, 2.3027931168927984, 2.3027072599378684, 2.3027684789592935, 2.3028134592508867, 2.3027673252558305, 2.302820044048762, 2.3028066946288286, 2.3027800099324374, 2.3027835902521168, 2.302811875181683, 2.3027824244256747, 2.302746089838319, 2.3028202743853554, 2.3027285781957336, 2.302797566025944, 2.3028268308962807, 2.3027699902906256, 2.302809785988371, 2.3027555397001365, 2.3027825719219144, 2.302761922448368, 2.30285996299679, 2.3027989864349365, 2.3028211411783253, 2.3028192055427423, 2.302804092229423, 2.302767583879374, 2.3027838428141707, 2.3028039972660905, 2.3027765912524725, 2.3028046862553744, 2.302787493851225, 2.3027788400650024, 2.302771463232525, 2.3028189287347307, 2.3027789188643633, 2.3028306516550354, 2.3027893143185114, 2.302763100397789, 2.3028056156837335, 2.3028451002250283, 2.302759299843998, 2.30277958360769, 2.3027948949296597, 2.3028111538644565, 2.302814002764427, 2.3027117595834246, 2.302838297213538, 2.302757661221391, 2.302806815858615, 2.302799804735992, 2.3028331146401873, 2.302799796654006, 2.3028113134836747, 2.302739254498886, 2.3027933937008096, 2.3027723461894665, 2.3028020737534862, 2.302829287819943, 2.302752135163647, 2.302754004122847, 2.302788534406888, 2.3028265460062833, 2.3027924137600397, 2.3028504282741222, 2.3027914419012556, 2.302807327044212, 2.3027368884975625, 2.3028058965327376, 2.3028004937252757, 2.3028001906508107, 2.3028029748948953, 2.302804185172259, 2.3028180700237466, 2.3027853339405384, 2.302772572485067, 2.3027453867055603, 2.3028045912920416, 2.3027666362665467, 2.3028501716710754, 2.3028139845799593, 2.3027966365975847, 2.3027993501242943, 2.3028047589932457, 2.3028172618251737, 2.302812887450396, 2.3027783793918157, 2.3027587906788973, 2.302760081776118, 2.302822747472989, 2.3027978488954446, 2.302776371018361, 2.3028077291230025, 2.3027669716689547, 2.302789237539647, 2.3027754476514914, 2.3028139300265553, 2.3027920298657176, 2.3027527797020086, 2.302823394031848, 2.3027214034128995, 2.3027514966867737, 2.302800455335843, 2.302799206669048, 2.302781040385618, 2.302778225834087, 2.3027814909563227, 2.3027632761809786, 2.3028552431171225, 2.3027962890721985, 2.302776368997865, 2.302825329667431, 2.302816694065676, 2.302784048904807, 2.3027570207240218, 2.3028029102390097, 2.3027990975622403, 2.302821028030525, 2.3028109477738203, 2.3027929956630127, 2.3027723744764166, 2.3027881666765375, 2.302776154825243, 2.3027789350283348, 2.3027891142893644, 2.30279143381927, 2.30279157525402, 2.3027311361442178, 2.3028307203519143, 2.302771525867915, 2.3028059935165666, 2.3028013281903026, 2.30276634531506, 2.302808991933273, 2.30280276274277, 2.3027773327746632, 2.302757661221391, 2.302742604481972, 2.3028026435334805, 2.302810479018648, 2.302796862893185, 2.3027917328527416, 2.3028054722284867, 2.3027935331150635, 2.3028095980822028, 2.302782697192693, 2.3027853581864957, 2.3027856107485496, 2.3028320316540993, 2.302770256996155, 2.302814827126972, 2.3027947676383844, 2.3028142270395313, 2.3027784662731623, 2.3027638358584905, 2.3028082403086, 2.3027324413849137, 2.3028168415619157, 2.302757539991605, 2.302805290383808, 2.3027645551552207, 2.3027926643015975, 2.3028103921373013, 2.3027661897368352, 2.302741420471062, 2.3028194560843, 2.302818421590126, 2.3027904215505566, 2.302788500058449, 2.3028052802813255, 2.3027657250226556, 2.302753410096896, 2.3027512340222374, 2.302765191611597, 2.3028034840599965, 2.302793054257409, 2.3028195429656466, 2.3027964325274453, 2.3027559094509837, 2.3028180356753074, 2.3027450674671237, 2.3028286695480347, 2.3027980125556557, 2.302838576042046, 2.3027902659723316, 2.3028141906705955, 2.302824410341554, 2.302785944130461, 2.3027997885720204, 2.302801447399592, 2.302778029845933, 2.302781058570086, 2.3027704792507624, 2.3028243860955966, 2.302754804239435, 2.302769654888218, 2.302768210233268, 2.302779518951804, 2.3027978994078557, 2.3027813212346224, 2.302763407513247, 2.3027878130896617, 2.3028017888634893, 2.3028088080680975, 2.3028082766775357, 2.302841552233292, 2.3027825820243963, 2.3027929108021623, 2.3028304657693637, 2.30275397987689, 2.30278774035179, 2.302782440589646, 2.3027377694340077, 2.3027925814612438, 2.302777722730475, 2.3027962587647517, 2.3027975438004833, 2.3027726916943565, 2.3027784986011053, 2.30274896096375, 2.3028020515280256, 2.3027757143570207, 2.302776645805876, 2.3027925915637257, 2.302758424969043, 2.3027969942254534, 2.3028137946532943, 2.3028273521843605, 2.3027862168974798, 2.3027957799070973, 2.3028452558032537, 2.3027453180086814, 2.3028003017781145, 2.3027917126477777, 2.302742018538006, 2.3027886899851135, 2.302784287323386, 2.3027722572876237, 2.302792933027623, 2.302818653947216, 2.3027750051627724, 2.3028225555258284, 2.3027503712702604, 2.3028019484827076, 2.3027943898055514, 2.3028195813550787, 2.3028297565751155, 2.3028053509987005, 2.3027781369322438, 2.3027784905191195, 2.3027999178837923, 2.3027727704937173, 2.302827028904931, 2.302731014914432, 2.3027804281751987, 2.302837236452911, 2.302801425174131, 2.302769426572121, 2.3028123762647983, 2.302819045923524, 2.3027247049040716, 2.302835672588672, 2.3027518442121604, 2.302788986998089, 2.302790991330551]\n",
            "Training Accuracies: [0.10036666666666667, 0.09836666666666667, 0.10078333333333334, 0.0981, 0.09825, 0.09728333333333333, 0.09846666666666666, 0.0993, 0.09886666666666667, 0.09948333333333333, 0.09971666666666666, 0.09646666666666667, 0.10038333333333334, 0.09756666666666666, 0.09811666666666667, 0.09696666666666667, 0.09723333333333334, 0.09933333333333333, 0.09866666666666667, 0.09685, 0.09903333333333333, 0.09873333333333334, 0.0995, 0.09831666666666666, 0.09828333333333333, 0.10008333333333333, 0.09885, 0.10038333333333334, 0.09766666666666667, 0.099, 0.09743333333333333, 0.0984, 0.09756666666666666, 0.09928333333333333, 0.09786666666666667, 0.09861666666666667, 0.09938333333333334, 0.10005, 0.09861666666666667, 0.09993333333333333, 0.09988333333333334, 0.09955, 0.09846666666666666, 0.10023333333333333, 0.0974, 0.09861666666666667, 0.10021666666666666, 0.09725, 0.09823333333333334, 0.09895, 0.09953333333333333, 0.09855, 0.09851666666666667, 0.0983, 0.10015, 0.09833333333333333, 0.09781666666666666, 0.09781666666666666, 0.09673333333333334, 0.09693333333333333, 0.09985, 0.10016666666666667, 0.09825, 0.0998, 0.09965, 0.09938333333333334, 0.09921666666666666, 0.09973333333333333, 0.09996666666666666, 0.09843333333333333, 0.09878333333333333, 0.09676666666666667, 0.09805, 0.09868333333333333, 0.0979, 0.09745, 0.09831666666666666, 0.0967, 0.09891666666666667, 0.09791666666666667, 0.09918333333333333, 0.09878333333333333, 0.0989, 0.09998333333333333, 0.0974, 0.09883333333333333, 0.09703333333333333, 0.09758333333333333, 0.09815, 0.0977, 0.09935, 0.09941666666666667, 0.09926666666666667, 0.09738333333333334, 0.09915, 0.09903333333333333, 0.09631666666666666, 0.09741666666666667, 0.0973, 0.099, 0.09805, 0.09761666666666667, 0.09978333333333333, 0.09775, 0.09985, 0.09828333333333333, 0.09638333333333333, 0.09758333333333333, 0.09813333333333334, 0.10001666666666667, 0.09978333333333333, 0.09933333333333333, 0.09816666666666667, 0.09916666666666667, 0.09695, 0.09766666666666667, 0.09653333333333333, 0.09831666666666666, 0.09885, 0.0965, 0.09961666666666667, 0.0991, 0.09893333333333333, 0.09936666666666667, 0.09871666666666666, 0.09761666666666667, 0.09938333333333334, 0.09846666666666666, 0.09945, 0.09718333333333333, 0.09866666666666667, 0.09868333333333333, 0.09928333333333333, 0.10111666666666666, 0.09718333333333333, 0.10105, 0.10028333333333334, 0.1007, 0.0967, 0.09928333333333333, 0.099, 0.09816666666666667, 0.09851666666666667, 0.09785, 0.1, 0.09943333333333333, 0.09736666666666667, 0.09843333333333333, 0.09973333333333333, 0.09956666666666666, 0.09851666666666667, 0.09853333333333333, 0.09823333333333334, 0.09801666666666667, 0.0967, 0.0986, 0.09753333333333333, 0.09875, 0.09813333333333334, 0.09876666666666667, 0.09903333333333333, 0.09981666666666666, 0.09908333333333333, 0.09833333333333333, 0.0999, 0.09863333333333334, 0.09666666666666666, 0.10033333333333333, 0.0987, 0.09543333333333333, 0.10028333333333334, 0.1001, 0.0986, 0.0994, 0.0987, 0.09806666666666666, 0.09868333333333333, 0.09646666666666667, 0.09851666666666667, 0.09756666666666666, 0.10121666666666666, 0.0995, 0.10013333333333334, 0.09925, 0.09816666666666667, 0.09595, 0.0986, 0.099, 0.09686666666666667, 0.1, 0.09993333333333333, 0.1004, 0.09891666666666667, 0.09806666666666666, 0.09825, 0.0996, 0.0981, 0.10015, 0.09893333333333333, 0.09988333333333334, 0.09743333333333333, 0.09828333333333333, 0.10078333333333334, 0.09886666666666667, 0.09641666666666666, 0.0986, 0.10038333333333334, 0.10155, 0.09883333333333333, 0.09993333333333333, 0.09856666666666666, 0.09908333333333333, 0.09925, 0.09901666666666667, 0.09961666666666667, 0.09931666666666666, 0.09878333333333333, 0.09943333333333333, 0.09781666666666666, 0.09906666666666666, 0.09786666666666667, 0.0972, 0.09971666666666666, 0.09645, 0.09835, 0.10013333333333334, 0.10038333333333334, 0.09763333333333334, 0.09828333333333333, 0.09781666666666666, 0.09858333333333333, 0.09846666666666666, 0.10005, 0.0979, 0.09998333333333333, 0.10081666666666667, 0.09941666666666667, 0.09995, 0.09883333333333333, 0.09988333333333334, 0.09888333333333334, 0.0988, 0.09891666666666667, 0.09808333333333333, 0.09951666666666667, 0.09753333333333333, 0.09871666666666666, 0.10056666666666667, 0.0985, 0.0993, 0.09715, 0.09956666666666666, 0.09946666666666666, 0.0981, 0.09903333333333333, 0.1, 0.09928333333333333, 0.09798333333333334, 0.09816666666666667, 0.09943333333333333, 0.10098333333333333, 0.09691666666666666, 0.0987, 0.10046666666666666, 0.09846666666666666, 0.09913333333333334, 0.09738333333333334, 0.09855, 0.09915, 0.09855, 0.09998333333333333, 0.10018333333333333, 0.09968333333333333, 0.09745, 0.09941666666666667, 0.09873333333333334, 0.097, 0.10058333333333333, 0.0984, 0.09798333333333334, 0.0967, 0.09795, 0.09735, 0.09748333333333334, 0.0973, 0.09951666666666667, 0.09846666666666666, 0.09695, 0.101, 0.09871666666666666, 0.09785, 0.09923333333333334, 0.09721666666666667, 0.09898333333333334, 0.09855, 0.10045, 0.09671666666666667, 0.09955, 0.09808333333333333, 0.09871666666666666]\n",
            "Validation Losses: [2.3026891231536863, 2.302784872055054, 2.3027655601501467, 2.3026789665222167, 2.302619755268097, 2.3027665257453918, 2.3026705622673034, 2.302816164493561, 2.302793192863464, 2.302684724330902, 2.3027389645576477, 2.302664589881897, 2.3027238965034487, 2.302710211277008, 2.3026519894599913, 2.3026283860206602, 2.302710211277008, 2.302721893787384, 2.30263090133667, 2.302732217311859, 2.3026202082633973, 2.3027875065803527, 2.302844786643982, 2.3027018308639526, 2.3026154160499575, 2.302802526950836, 2.3026904344558714, 2.3027087807655335, 2.3027006864547728, 2.302687704563141, 2.3027087092399596, 2.302605485916138, 2.3026618003845214, 2.302670729160309, 2.302721118927002, 2.302725946903229, 2.3028544902801515, 2.3027979850769045, 2.3027246475219725, 2.3027979493141175, 2.3027429223060607, 2.302674400806427, 2.302656018733978, 2.3027064919471742, 2.302676773071289, 2.302716886997223, 2.3027238249778748, 2.302648937702179, 2.3027002692222593, 2.302649438381195, 2.302650511264801, 2.3027477860450745, 2.302664041519165, 2.3027560114860535, 2.302693009376526, 2.302827513217926, 2.3025873422622682, 2.3027302861213683, 2.3027180433273315, 2.3027151942253115, 2.302711284160614, 2.3026639342308046, 2.302733898162842, 2.3026403903961183, 2.3027599096298217, 2.3026211977005007, 2.302736794948578, 2.302705228328705, 2.3027443408966066, 2.302679884433746, 2.302698040008545, 2.302684473991394, 2.302664804458618, 2.3026930809021, 2.3026878237724304, 2.30261367559433, 2.302769589424133, 2.3028698801994323, 2.302800953388214, 2.302611744403839, 2.3026371359825135, 2.3027928352355955, 2.3026890635490416, 2.302718794345856, 2.3026722073554993, 2.302701508998871, 2.3026369214057922, 2.302739715576172, 2.302659606933594, 2.302840828895569, 2.3027904510498045, 2.30271030664444, 2.3026718854904176, 2.302676963806152, 2.3027829051017763, 2.3027129888534548, 2.302765667438507, 2.3026524424552917, 2.3027830958366393, 2.3026004314422606, 2.3027394652366637, 2.3026583671569822, 2.3028021574020388, 2.302648091316223, 2.30260351896286, 2.3026510000228884, 2.3027112245559693, 2.302763211727142, 2.3027401685714723, 2.3027005076408384, 2.3026690602302553, 2.302867078781128, 2.302712821960449, 2.3028502225875855, 2.3026451468467712, 2.3027096152305604, 2.3026540756225584, 2.3027091026306152, 2.302721989154816, 2.3027114868164062, 2.302645194530487, 2.3026429653167724, 2.302665877342224, 2.302675747871399, 2.302667760848999, 2.302728235721588, 2.302743875980377, 2.302687132358551, 2.302690589427948, 2.302632820606232, 2.3027182936668398, 2.30266010761261, 2.302710735797882, 2.3027938842773437, 2.302693796157837, 2.3026872634887696, 2.3027613878250124, 2.3027184128761293, 2.3027358293533324, 2.3026214241981506, 2.302629363536835, 2.3026086568832396, 2.302802336215973, 2.3026452898979186, 2.302620255947113, 2.3026760697364805, 2.302679646015167, 2.302611267566681, 2.3026870608329775, 2.302689790725708, 2.302745485305786, 2.3027675271034242, 2.3026673793792725, 2.3026315450668333, 2.3026436448097227, 2.302650499343872, 2.302675426006317, 2.3026336431503296, 2.302656102180481, 2.302686870098114, 2.302697467803955, 2.302658033370972, 2.302802526950836, 2.3026944041252135, 2.3027217388153076, 2.302804934978485, 2.3026899933815, 2.3027300238609314, 2.3026798605918883, 2.30266615152359, 2.302721107006073, 2.3027204990386965, 2.3027966141700746, 2.302684485912323, 2.3027214884757994, 2.3027060508728026, 2.30275319814682, 2.302690875530243, 2.3026665091514587, 2.302609348297119, 2.3027019619941713, 2.3028539538383486, 2.3027900099754333, 2.302599513530731, 2.3027509808540345, 2.302619123458862, 2.302650249004364, 2.3026440858840944, 2.3026866555213927, 2.3027594685554504, 2.302711284160614, 2.302764630317688, 2.3027012228965758, 2.302732014656067, 2.3026644825935363, 2.302732253074646, 2.3026541233062745, 2.3026416182518004, 2.3027288913726807, 2.3027106881141663, 2.302669906616211, 2.3026885747909547, 2.302724552154541, 2.302688443660736, 2.302609121799469, 2.302720606327057, 2.3027058124542235, 2.3027385592460634, 2.3028432965278625, 2.3026742935180664, 2.302735483646393, 2.3026401877403258, 2.302614140510559, 2.302681529521942, 2.3026408910751344, 2.3027060270309447, 2.302671694755554, 2.3026615738868714, 2.302658033370972, 2.3026611685752867, 2.3026687502861023, 2.3025856733322145, 2.3027232766151426, 2.302737665176392, 2.3028026103973387, 2.3026283144950868, 2.3026670694351195, 2.302699315547943, 2.3026734828948974, 2.3028008699417115, 2.3026083111763, 2.3027042746543884, 2.302625024318695, 2.302656042575836, 2.3026501297950746, 2.3027431845664976, 2.302752125263214, 2.30264390707016, 2.302647590637207, 2.302625906467438, 2.3026275515556334, 2.302711522579193, 2.302726888656616, 2.302621293067932, 2.302626371383667, 2.302659571170807, 2.302686297893524, 2.3027236104011535, 2.3026994705200194, 2.302728068828583, 2.3026693224906922, 2.3026405572891235, 2.3027176737785338, 2.3027370810508727, 2.3026854038238525, 2.302674043178558, 2.3026978254318236, 2.3028204917907713, 2.3026853442192077, 2.302835738658905, 2.302648591995239, 2.3027036666870115, 2.3026315689086916, 2.3026842355728148, 2.3027757167816163, 2.302595818042755, 2.3027646780014037, 2.3026958107948303, 2.3026532888412476, 2.302794301509857, 2.3026870012283327, 2.3026037573814393, 2.3027078032493593, 2.302686405181885, 2.3026801705360413, 2.302719068527222, 2.3027003169059754, 2.302786707878113, 2.3027176022529603, 2.3026686668395997, 2.3027124047279357, 2.302663516998291, 2.302676272392273, 2.3027171015739443, 2.3026976346969605, 2.3027552008628844, 2.302668845653534, 2.3026674032211303, 2.3027023911476134, 2.3027196288108827, 2.3026621460914614, 2.3027142524719237, 2.3027487277984617, 2.302585208415985, 2.3026007652282714, 2.3027368068695067, 2.302629625797272, 2.3027061223983765, 2.3026033282279967, 2.30272775888443]\n",
            "Validation Accuracies: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1003, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0996, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1089, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n"
          ]
        }
      ]
    }
  ]
}